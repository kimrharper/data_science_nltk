{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, SparsePCA, NMF\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "np.seterr(divide='warn', invalid='warn'); sns.set_style(\"whitegrid\");warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Bosch Manufacturing Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ [Ryan Harper](www.kimrharper.com) <br><br>\n",
    "__Data Source:__ [Bosch Dataset via Kaggle](https://www.kaggle.com/c/bosch-production-line-performance/data) <br> <br>\n",
    "__Background:__ Bosch is a home appliance and industrial tools manufacturing company. In 2017, Bosch supplied Kaggle.com with manufacturing data to promote a competition. The goal of the competition was to determine factors that influence whether or not the product passes the final response stage of manufacturing and to predict which products are likely to fail based on this manufacturing process.<br> <br>\n",
    "__The Data:__ Early exploration of this data will use a subset of the big data provided by Bosch. The data is provided by [Hitesh, John, and Matthew via PDX Data Science Meetup](https://www.meetup.com/Portland-Data-Science-Group/events/257370691/). The data subset is divided into 2 groups of 3 files (3 training, 3 test). Each group has one csv file each for numerical features ('numeric'), dates ('date'), and the manufacturing path ('cat'). The data subset includes a larger percentage of products that failed the response test, but not much more is known about this subsampling method.<br><br>\n",
    "__Assumptions:__ ID # represents a specific product and that there is only one product. The differences in assembly are due to customization and/or differences between lines.<br><br>\n",
    "__Goal:__ Predict which products will fail the response test. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.59 s, sys: 1.21 s, total: 10.8 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import numerical data\n",
    "mf_num_data = pd.read_csv('bosch_small_data/train_numeric.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    names = list(df.columns)\n",
    "    impute_constant = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "    return impute_constant.fit_transform(df), names\n",
    "\n",
    "def visualize_data(pipeline,dimred):\n",
    "    feature_plot = list(zip(features, pipeline.named_steps[dimred].components_[0]))\n",
    "    feature_plot = pd.DataFrame(data=feature_plot)\n",
    "    feature_plot = pd.DataFrame(feature_plot.sort_values(1, ascending=False).iloc[0:10])\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title('Ordered by variance')\n",
    "    sns.barplot(x=0, y=1, data=feature_plot, palette=sns.color_palette(\"cool\"))\n",
    "    plt.ylim(feature_plot[1].min(),feature_plot[1].max())\n",
    "    plt.savefig(dimred+'-top10.png')\n",
    "    plt.show()\n",
    "     \n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.title('Component Variance')\n",
    "    plt.plot(pipeline.named_steps[dimred].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Split data to x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, features = process_data(mf_num_data.iloc[:,1:-1])\n",
    "y = mf_num_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression(random_state=42, solver='liblinear', multi_class='ovr')\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=200, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    # the reduce_dim stage is populated by the param_grid\n",
    "    ('reduce_dim', None),\n",
    "    ('model', None)\n",
    "])\n",
    "\n",
    "# Candidates = 40 (5 Components * 2 RF Depth * 4 Feature Selection Methods)\n",
    "\n",
    "N_FEATURES_OPTIONS = [500, 700, len(X[0,:])]\n",
    "RF_DEPTH = [10, 50, 100]\n",
    "RF_ESTIM =  [10, 50, 100]\n",
    "LOGR_SOLVER = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "REDUCER_LABELS = ['PCA', 'TSVD', 'NMF','KBest']\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA()],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'model': [RandomForestClassifier()],\n",
    "        'model__max_depth': RF_DEPTH,\n",
    "        'model__n_estimators': RF_ESTIM,\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [PCA()],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'model': [LogisticRegression(solver='liblinear', multi_class='ovr')],\n",
    "        'model__solver': LOGR_SOLVER\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=4, scoring=make_scorer(matthews_corrcoef), verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed: 14.7min\n"
     ]
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_scores = np.array(grid.cv_results_['mean_test_score']).reshape(4,3,3)\n",
    "grid_results = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "columns = ['param_model',\n",
    "           'param_reduce_dim__n_components',\n",
    "           'param_model__max_depth',\n",
    "           'param_model__n_estimators',\n",
    "           'mean_test_score',\n",
    "           'rank_test_score',\n",
    "           'mean_fit_time']\n",
    "grid_results = grid_results[columns]\n",
    "grid_results['param_model']=grid_results['param_model'].apply(lambda val: str(val).split('(')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model</th>\n",
       "      <th>param_model__max_depth</th>\n",
       "      <th>param_model__n_estimators</th>\n",
       "      <th>param_reduce_dim</th>\n",
       "      <th>param_reduce_dim__n_components</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.470687</td>\n",
       "      <td>0.072315</td>\n",
       "      <td>2.561092</td>\n",
       "      <td>0.087748</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
       "      <td>968</td>\n",
       "      <td>{'model': RandomForestClassifier(bootstrap=Tru...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172146</td>\n",
       "      <td>0.159997</td>\n",
       "      <td>0.173855</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>2</td>\n",
       "      <td>0.230619</td>\n",
       "      <td>0.276971</td>\n",
       "      <td>0.253364</td>\n",
       "      <td>0.253651</td>\n",
       "      <td>0.018924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.734237</td>\n",
       "      <td>19.919648</td>\n",
       "      <td>1.520189</td>\n",
       "      <td>0.122290</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
       "      <td>968</td>\n",
       "      <td>{'model': LogisticRegression(C=1.0, class_weig...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191504</td>\n",
       "      <td>0.192631</td>\n",
       "      <td>0.201561</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.198606</td>\n",
       "      <td>0.219803</td>\n",
       "      <td>0.215020</td>\n",
       "      <td>0.211143</td>\n",
       "      <td>0.009078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      54.470687      0.072315         2.561092        0.087748   \n",
       "1      50.734237     19.919648         1.520189        0.122290   \n",
       "\n",
       "                                         param_model param_model__max_depth  \\\n",
       "0  RandomForestClassifier(bootstrap=True, class_w...                     10   \n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...                    NaN   \n",
       "\n",
       "  param_model__n_estimators  \\\n",
       "0                        10   \n",
       "1                       NaN   \n",
       "\n",
       "                                    param_reduce_dim  \\\n",
       "0  PCA(copy=True, iterated_power='auto', n_compon...   \n",
       "1  PCA(copy=True, iterated_power='auto', n_compon...   \n",
       "\n",
       "  param_reduce_dim__n_components  \\\n",
       "0                            968   \n",
       "1                            968   \n",
       "\n",
       "                                              params       ...         \\\n",
       "0  {'model': RandomForestClassifier(bootstrap=Tru...       ...          \n",
       "1  {'model': LogisticRegression(C=1.0, class_weig...       ...          \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.172146           0.159997         0.173855        0.012073   \n",
       "1           0.191504           0.192631         0.201561        0.013434   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                2            0.230619            0.276971   \n",
       "1                1            0.198606            0.219803   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "0            0.253364          0.253651         0.018924  \n",
       "1            0.215020          0.211143         0.009078  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
