{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DCGAN on MNIST using Keras\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "Dependencies: tensorflow 1.0 and keras 2.0\n",
    "Usage: python3 dcgan_mnist.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    # (Wâˆ’F+2P)/S+1\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "        # In: 28 x 28 x 1, depth = 1\n",
    "        # Out: 14 x 14 x 1, depth=64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.45\n",
    "        depth = 64+64+64+64\n",
    "        dim = 7\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DT (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 2.260012, acc: 0.505859]  [A loss: 0.364594, acc: 1.000000]\n",
      "1: [D loss: 4.236373, acc: 0.500000]  [A loss: 0.186224, acc: 1.000000]\n",
      "2: [D loss: 3.771472, acc: 0.500000]  [A loss: 0.152484, acc: 1.000000]\n",
      "3: [D loss: 3.265887, acc: 0.500000]  [A loss: 0.145958, acc: 1.000000]\n",
      "4: [D loss: 2.897113, acc: 0.500000]  [A loss: 0.147827, acc: 1.000000]\n",
      "5: [D loss: 2.583673, acc: 0.500000]  [A loss: 0.149908, acc: 1.000000]\n",
      "6: [D loss: 2.366109, acc: 0.500000]  [A loss: 0.163878, acc: 1.000000]\n",
      "7: [D loss: 2.173348, acc: 0.500000]  [A loss: 0.170792, acc: 1.000000]\n",
      "8: [D loss: 2.020418, acc: 0.500000]  [A loss: 0.184210, acc: 1.000000]\n",
      "9: [D loss: 1.847067, acc: 0.500000]  [A loss: 0.192704, acc: 1.000000]\n",
      "10: [D loss: 1.685375, acc: 0.500000]  [A loss: 0.200233, acc: 1.000000]\n",
      "11: [D loss: 1.583161, acc: 0.500000]  [A loss: 0.210266, acc: 1.000000]\n",
      "12: [D loss: 1.482543, acc: 0.500000]  [A loss: 0.227311, acc: 1.000000]\n",
      "13: [D loss: 1.379356, acc: 0.500000]  [A loss: 0.234227, acc: 1.000000]\n",
      "14: [D loss: 1.331116, acc: 0.500000]  [A loss: 0.241249, acc: 1.000000]\n",
      "15: [D loss: 1.254941, acc: 0.500000]  [A loss: 0.251753, acc: 1.000000]\n",
      "16: [D loss: 1.185575, acc: 0.500000]  [A loss: 0.258887, acc: 1.000000]\n",
      "17: [D loss: 1.138618, acc: 0.500000]  [A loss: 0.271976, acc: 1.000000]\n",
      "18: [D loss: 1.103764, acc: 0.500000]  [A loss: 0.278498, acc: 1.000000]\n",
      "19: [D loss: 1.055978, acc: 0.500000]  [A loss: 0.289276, acc: 1.000000]\n",
      "20: [D loss: 1.009285, acc: 0.500000]  [A loss: 0.291683, acc: 1.000000]\n",
      "21: [D loss: 0.983478, acc: 0.500000]  [A loss: 0.296866, acc: 1.000000]\n",
      "22: [D loss: 0.959354, acc: 0.500000]  [A loss: 0.319781, acc: 1.000000]\n",
      "23: [D loss: 0.953848, acc: 0.500000]  [A loss: 0.337137, acc: 1.000000]\n",
      "24: [D loss: 0.925002, acc: 0.500000]  [A loss: 0.346033, acc: 1.000000]\n",
      "25: [D loss: 0.895714, acc: 0.500000]  [A loss: 0.364541, acc: 1.000000]\n",
      "26: [D loss: 0.885378, acc: 0.500000]  [A loss: 0.391377, acc: 1.000000]\n",
      "27: [D loss: 0.869592, acc: 0.500000]  [A loss: 0.408306, acc: 1.000000]\n",
      "28: [D loss: 0.837571, acc: 0.500000]  [A loss: 0.445223, acc: 1.000000]\n",
      "29: [D loss: 0.828726, acc: 0.500000]  [A loss: 0.444849, acc: 1.000000]\n",
      "30: [D loss: 0.774066, acc: 0.500000]  [A loss: 0.463689, acc: 1.000000]\n",
      "31: [D loss: 0.783337, acc: 0.500000]  [A loss: 0.509577, acc: 0.992188]\n",
      "32: [D loss: 0.775747, acc: 0.500000]  [A loss: 0.535092, acc: 0.933594]\n",
      "33: [D loss: 0.768466, acc: 0.500000]  [A loss: 0.594262, acc: 0.851562]\n",
      "34: [D loss: 0.743855, acc: 0.500000]  [A loss: 0.638895, acc: 0.699219]\n",
      "35: [D loss: 0.734391, acc: 0.500000]  [A loss: 0.716475, acc: 0.468750]\n",
      "36: [D loss: 0.743840, acc: 0.500000]  [A loss: 0.794831, acc: 0.304688]\n",
      "37: [D loss: 0.699316, acc: 0.500000]  [A loss: 0.872414, acc: 0.167969]\n",
      "38: [D loss: 0.690030, acc: 0.500000]  [A loss: 1.001446, acc: 0.066406]\n",
      "39: [D loss: 0.657705, acc: 0.500000]  [A loss: 1.173820, acc: 0.023438]\n",
      "40: [D loss: 0.627782, acc: 0.503906]  [A loss: 1.324235, acc: 0.019531]\n",
      "41: [D loss: 0.579899, acc: 0.500000]  [A loss: 1.463481, acc: 0.000000]\n",
      "42: [D loss: 0.557054, acc: 0.509766]  [A loss: 1.755358, acc: 0.003906]\n",
      "43: [D loss: 0.508452, acc: 0.525391]  [A loss: 1.325874, acc: 0.011719]\n",
      "44: [D loss: 0.559348, acc: 0.511719]  [A loss: 2.051030, acc: 0.000000]\n",
      "45: [D loss: 0.569986, acc: 0.568359]  [A loss: 0.102463, acc: 1.000000]\n",
      "46: [D loss: 1.250708, acc: 0.500000]  [A loss: 0.982770, acc: 0.125000]\n",
      "47: [D loss: 0.581623, acc: 0.507812]  [A loss: 1.558801, acc: 0.000000]\n",
      "48: [D loss: 0.464277, acc: 0.568359]  [A loss: 1.783885, acc: 0.000000]\n",
      "49: [D loss: 0.474998, acc: 0.585938]  [A loss: 0.624399, acc: 0.671875]\n",
      "50: [D loss: 0.660459, acc: 0.501953]  [A loss: 1.597505, acc: 0.000000]\n",
      "51: [D loss: 0.469834, acc: 0.548828]  [A loss: 1.892653, acc: 0.000000]\n",
      "52: [D loss: 0.443376, acc: 0.585938]  [A loss: 1.640350, acc: 0.000000]\n",
      "53: [D loss: 0.551278, acc: 0.583984]  [A loss: 0.234561, acc: 1.000000]\n",
      "54: [D loss: 0.956395, acc: 0.500000]  [A loss: 1.700746, acc: 0.000000]\n",
      "55: [D loss: 0.543728, acc: 0.541016]  [A loss: 2.441293, acc: 0.000000]\n",
      "56: [D loss: 0.651288, acc: 0.697266]  [A loss: 0.389927, acc: 0.960938]\n",
      "57: [D loss: 0.869292, acc: 0.500000]  [A loss: 2.485783, acc: 0.000000]\n",
      "58: [D loss: 0.505236, acc: 0.673828]  [A loss: 0.045455, acc: 1.000000]\n",
      "59: [D loss: 1.653775, acc: 0.500000]  [A loss: 1.200265, acc: 0.074219]\n",
      "60: [D loss: 0.570915, acc: 0.523438]  [A loss: 2.298244, acc: 0.000000]\n",
      "61: [D loss: 0.434622, acc: 0.638672]  [A loss: 2.455843, acc: 0.000000]\n",
      "62: [D loss: 0.361605, acc: 0.750000]  [A loss: 2.561584, acc: 0.000000]\n",
      "63: [D loss: 0.540792, acc: 0.759766]  [A loss: 0.003901, acc: 1.000000]\n",
      "64: [D loss: 2.608371, acc: 0.500000]  [A loss: 0.205683, acc: 1.000000]\n",
      "65: [D loss: 1.041413, acc: 0.500000]  [A loss: 0.893280, acc: 0.226562]\n",
      "66: [D loss: 0.605762, acc: 0.517578]  [A loss: 1.573697, acc: 0.000000]\n",
      "67: [D loss: 0.516761, acc: 0.560547]  [A loss: 0.481678, acc: 0.898438]\n",
      "68: [D loss: 0.814282, acc: 0.500000]  [A loss: 1.558667, acc: 0.011719]\n",
      "69: [D loss: 0.533933, acc: 0.558594]  [A loss: 1.873487, acc: 0.000000]\n",
      "70: [D loss: 0.491302, acc: 0.560547]  [A loss: 2.016746, acc: 0.000000]\n",
      "71: [D loss: 0.458053, acc: 0.585938]  [A loss: 0.311243, acc: 0.988281]\n",
      "72: [D loss: 0.939340, acc: 0.500000]  [A loss: 1.810257, acc: 0.000000]\n",
      "73: [D loss: 0.518419, acc: 0.552734]  [A loss: 1.959453, acc: 0.000000]\n",
      "74: [D loss: 0.424224, acc: 0.568359]  [A loss: 1.234995, acc: 0.039062]\n",
      "75: [D loss: 0.463402, acc: 0.546875]  [A loss: 1.280570, acc: 0.035156]\n",
      "76: [D loss: 0.472829, acc: 0.542969]  [A loss: 1.518480, acc: 0.000000]\n",
      "77: [D loss: 0.414995, acc: 0.599609]  [A loss: 1.302892, acc: 0.027344]\n",
      "78: [D loss: 0.460952, acc: 0.552734]  [A loss: 1.648662, acc: 0.003906]\n",
      "79: [D loss: 0.380485, acc: 0.654297]  [A loss: 1.317417, acc: 0.015625]\n",
      "80: [D loss: 0.412691, acc: 0.607422]  [A loss: 1.440160, acc: 0.019531]\n",
      "81: [D loss: 0.424755, acc: 0.654297]  [A loss: 1.567425, acc: 0.007812]\n",
      "82: [D loss: 0.391269, acc: 0.750000]  [A loss: 0.307643, acc: 1.000000]\n",
      "83: [D loss: 0.943343, acc: 0.500000]  [A loss: 2.096276, acc: 0.000000]\n",
      "84: [D loss: 0.377769, acc: 0.748047]  [A loss: 0.318114, acc: 0.996094]\n",
      "85: [D loss: 0.950555, acc: 0.500000]  [A loss: 2.172351, acc: 0.000000]\n",
      "86: [D loss: 0.372004, acc: 0.722656]  [A loss: 0.264525, acc: 1.000000]\n",
      "87: [D loss: 1.016214, acc: 0.500000]  [A loss: 1.809558, acc: 0.000000]\n",
      "88: [D loss: 0.445232, acc: 0.664062]  [A loss: 0.330270, acc: 1.000000]\n",
      "89: [D loss: 1.033423, acc: 0.500000]  [A loss: 2.318332, acc: 0.000000]\n",
      "90: [D loss: 0.411858, acc: 0.654297]  [A loss: 2.090786, acc: 0.000000]\n",
      "91: [D loss: 0.411195, acc: 0.763672]  [A loss: 1.905061, acc: 0.000000]\n",
      "92: [D loss: 0.390912, acc: 0.800781]  [A loss: 0.080829, acc: 1.000000]\n",
      "93: [D loss: 1.531466, acc: 0.500000]  [A loss: 0.751474, acc: 0.398438]\n",
      "94: [D loss: 0.683063, acc: 0.507812]  [A loss: 2.208296, acc: 0.000000]\n",
      "95: [D loss: 0.398686, acc: 0.683594]  [A loss: 2.083037, acc: 0.000000]\n",
      "96: [D loss: 0.385823, acc: 0.751953]  [A loss: 2.134873, acc: 0.000000]\n",
      "97: [D loss: 0.373007, acc: 0.767578]  [A loss: 2.000653, acc: 0.000000]\n",
      "98: [D loss: 0.336779, acc: 0.777344]  [A loss: 2.206742, acc: 0.000000]\n",
      "99: [D loss: 0.386248, acc: 0.859375]  [A loss: 0.086578, acc: 1.000000]\n",
      "100: [D loss: 1.574776, acc: 0.500000]  [A loss: 0.678208, acc: 0.585938]\n",
      "101: [D loss: 0.696179, acc: 0.505859]  [A loss: 2.157323, acc: 0.000000]\n",
      "102: [D loss: 0.390802, acc: 0.687500]  [A loss: 0.232564, acc: 1.000000]\n",
      "103: [D loss: 1.214175, acc: 0.500000]  [A loss: 1.702048, acc: 0.000000]\n",
      "104: [D loss: 0.542385, acc: 0.554688]  [A loss: 2.569145, acc: 0.000000]\n",
      "105: [D loss: 0.399058, acc: 0.683594]  [A loss: 2.345325, acc: 0.000000]\n",
      "106: [D loss: 0.332261, acc: 0.804688]  [A loss: 1.994232, acc: 0.000000]\n",
      "107: [D loss: 0.347156, acc: 0.763672]  [A loss: 2.086878, acc: 0.000000]\n",
      "108: [D loss: 0.379143, acc: 0.779297]  [A loss: 2.409605, acc: 0.000000]\n",
      "109: [D loss: 0.413368, acc: 0.832031]  [A loss: 0.166583, acc: 1.000000]\n",
      "110: [D loss: 1.375813, acc: 0.500000]  [A loss: 1.703210, acc: 0.003906]\n",
      "111: [D loss: 0.549353, acc: 0.556641]  [A loss: 3.235381, acc: 0.000000]\n",
      "112: [D loss: 0.330108, acc: 0.861328]  [A loss: 0.087818, acc: 1.000000]\n",
      "113: [D loss: 1.641458, acc: 0.500000]  [A loss: 1.376675, acc: 0.023438]\n",
      "114: [D loss: 0.705510, acc: 0.515625]  [A loss: 3.395154, acc: 0.000000]\n",
      "115: [D loss: 0.398499, acc: 0.748047]  [A loss: 2.888116, acc: 0.000000]\n",
      "116: [D loss: 0.371858, acc: 0.789062]  [A loss: 1.054499, acc: 0.160156]\n",
      "117: [D loss: 0.805959, acc: 0.517578]  [A loss: 4.388688, acc: 0.000000]\n",
      "118: [D loss: 0.432459, acc: 0.902344]  [A loss: 1.074133, acc: 0.160156]\n",
      "119: [D loss: 0.807882, acc: 0.511719]  [A loss: 4.109769, acc: 0.000000]\n",
      "120: [D loss: 0.364494, acc: 0.917969]  [A loss: 0.156600, acc: 1.000000]\n",
      "121: [D loss: 1.356967, acc: 0.500000]  [A loss: 2.049031, acc: 0.000000]\n",
      "122: [D loss: 0.510530, acc: 0.617188]  [A loss: 3.369594, acc: 0.000000]\n",
      "123: [D loss: 0.331005, acc: 0.810547]  [A loss: 2.977426, acc: 0.000000]\n",
      "124: [D loss: 0.409630, acc: 0.843750]  [A loss: 0.147846, acc: 1.000000]\n",
      "125: [D loss: 1.434861, acc: 0.500000]  [A loss: 2.197845, acc: 0.000000]\n",
      "126: [D loss: 0.529411, acc: 0.613281]  [A loss: 3.462607, acc: 0.000000]\n",
      "127: [D loss: 0.328348, acc: 0.816406]  [A loss: 0.501282, acc: 0.824219]\n",
      "128: [D loss: 1.150855, acc: 0.500000]  [A loss: 4.058573, acc: 0.000000]\n",
      "129: [D loss: 0.329324, acc: 0.853516]  [A loss: 2.926027, acc: 0.000000]\n",
      "130: [D loss: 0.376765, acc: 0.742188]  [A loss: 3.052580, acc: 0.000000]\n",
      "131: [D loss: 0.401165, acc: 0.796875]  [A loss: 3.242456, acc: 0.000000]\n",
      "132: [D loss: 0.349765, acc: 0.808594]  [A loss: 2.991113, acc: 0.000000]\n",
      "133: [D loss: 0.362445, acc: 0.847656]  [A loss: 3.122256, acc: 0.000000]\n",
      "134: [D loss: 0.331928, acc: 0.902344]  [A loss: 0.071585, acc: 1.000000]\n",
      "135: [D loss: 1.725997, acc: 0.500000]  [A loss: 1.927098, acc: 0.000000]\n",
      "136: [D loss: 0.629032, acc: 0.580078]  [A loss: 4.661807, acc: 0.000000]\n",
      "137: [D loss: 0.410004, acc: 0.953125]  [A loss: 0.118556, acc: 1.000000]\n",
      "138: [D loss: 1.417059, acc: 0.500000]  [A loss: 1.632732, acc: 0.042969]\n",
      "139: [D loss: 0.751996, acc: 0.525391]  [A loss: 4.072949, acc: 0.000000]\n",
      "140: [D loss: 0.284334, acc: 0.908203]  [A loss: 2.648845, acc: 0.000000]\n",
      "141: [D loss: 0.410247, acc: 0.699219]  [A loss: 0.575331, acc: 0.710938]\n",
      "142: [D loss: 1.150076, acc: 0.503906]  [A loss: 4.733970, acc: 0.000000]\n",
      "143: [D loss: 0.298924, acc: 0.904297]  [A loss: 3.093946, acc: 0.000000]\n",
      "144: [D loss: 0.352518, acc: 0.785156]  [A loss: 3.444950, acc: 0.000000]\n",
      "145: [D loss: 0.308835, acc: 0.818359]  [A loss: 3.360425, acc: 0.000000]\n",
      "146: [D loss: 0.304935, acc: 0.832031]  [A loss: 3.334865, acc: 0.000000]\n",
      "147: [D loss: 0.287534, acc: 0.833984]  [A loss: 3.379260, acc: 0.000000]\n",
      "148: [D loss: 0.262879, acc: 0.884766]  [A loss: 3.404626, acc: 0.000000]\n",
      "149: [D loss: 0.265101, acc: 0.882812]  [A loss: 3.430874, acc: 0.000000]\n",
      "150: [D loss: 0.247853, acc: 0.898438]  [A loss: 3.576906, acc: 0.000000]\n",
      "151: [D loss: 0.208636, acc: 0.947266]  [A loss: 3.251345, acc: 0.003906]\n",
      "152: [D loss: 0.253575, acc: 0.886719]  [A loss: 3.987134, acc: 0.000000]\n",
      "153: [D loss: 0.180193, acc: 0.984375]  [A loss: 2.997912, acc: 0.000000]\n",
      "154: [D loss: 0.309702, acc: 0.890625]  [A loss: 4.032698, acc: 0.000000]\n",
      "155: [D loss: 0.223116, acc: 0.978516]  [A loss: 0.007632, acc: 1.000000]\n",
      "156: [D loss: 2.414001, acc: 0.500000]  [A loss: 0.497548, acc: 0.800781]\n",
      "157: [D loss: 1.131945, acc: 0.500000]  [A loss: 4.410419, acc: 0.000000]\n",
      "158: [D loss: 0.214306, acc: 0.947266]  [A loss: 2.979259, acc: 0.000000]\n",
      "159: [D loss: 0.332436, acc: 0.806641]  [A loss: 3.897081, acc: 0.000000]\n",
      "160: [D loss: 0.249866, acc: 0.919922]  [A loss: 3.582963, acc: 0.000000]\n",
      "161: [D loss: 0.243923, acc: 0.884766]  [A loss: 3.670298, acc: 0.000000]\n",
      "162: [D loss: 0.237120, acc: 0.912109]  [A loss: 3.863583, acc: 0.000000]\n",
      "163: [D loss: 0.274211, acc: 0.925781]  [A loss: 0.100832, acc: 1.000000]\n",
      "164: [D loss: 1.646320, acc: 0.500000]  [A loss: 5.048581, acc: 0.000000]\n",
      "165: [D loss: 0.140569, acc: 0.980469]  [A loss: 3.102091, acc: 0.000000]\n",
      "166: [D loss: 0.387586, acc: 0.751953]  [A loss: 4.700520, acc: 0.000000]\n",
      "167: [D loss: 0.155470, acc: 0.968750]  [A loss: 3.388916, acc: 0.000000]\n",
      "168: [D loss: 0.319734, acc: 0.822266]  [A loss: 5.125115, acc: 0.000000]\n",
      "169: [D loss: 0.171630, acc: 0.974609]  [A loss: 3.590824, acc: 0.000000]\n",
      "170: [D loss: 0.289560, acc: 0.843750]  [A loss: 4.937436, acc: 0.000000]\n",
      "171: [D loss: 0.200012, acc: 0.968750]  [A loss: 3.880543, acc: 0.000000]\n",
      "172: [D loss: 0.219144, acc: 0.917969]  [A loss: 4.477916, acc: 0.000000]\n",
      "173: [D loss: 0.148589, acc: 0.978516]  [A loss: 4.054818, acc: 0.000000]\n",
      "174: [D loss: 0.183012, acc: 0.947266]  [A loss: 4.614580, acc: 0.000000]\n",
      "175: [D loss: 0.215550, acc: 0.968750]  [A loss: 0.007641, acc: 1.000000]\n",
      "176: [D loss: 2.798960, acc: 0.500000]  [A loss: 1.582879, acc: 0.070312]\n",
      "177: [D loss: 1.097341, acc: 0.511719]  [A loss: 7.346068, acc: 0.000000]\n",
      "178: [D loss: 0.197059, acc: 0.986328]  [A loss: 0.040780, acc: 1.000000]\n",
      "179: [D loss: 2.067081, acc: 0.500000]  [A loss: 1.921811, acc: 0.042969]\n",
      "180: [D loss: 1.030119, acc: 0.525391]  [A loss: 5.356496, acc: 0.000000]\n",
      "181: [D loss: 0.271790, acc: 0.841797]  [A loss: 4.100656, acc: 0.000000]\n",
      "182: [D loss: 0.498944, acc: 0.707031]  [A loss: 5.369396, acc: 0.000000]\n",
      "183: [D loss: 0.244646, acc: 0.878906]  [A loss: 4.355268, acc: 0.000000]\n",
      "184: [D loss: 0.494140, acc: 0.707031]  [A loss: 6.391695, acc: 0.000000]\n",
      "185: [D loss: 0.162151, acc: 0.949219]  [A loss: 4.112408, acc: 0.000000]\n",
      "186: [D loss: 0.419095, acc: 0.757812]  [A loss: 6.073317, acc: 0.000000]\n",
      "187: [D loss: 0.142536, acc: 0.951172]  [A loss: 4.097305, acc: 0.000000]\n",
      "188: [D loss: 0.462335, acc: 0.734375]  [A loss: 6.776503, acc: 0.000000]\n",
      "189: [D loss: 0.184121, acc: 0.980469]  [A loss: 3.940604, acc: 0.000000]\n",
      "190: [D loss: 0.515809, acc: 0.695312]  [A loss: 7.998430, acc: 0.000000]\n",
      "191: [D loss: 0.142633, acc: 0.990234]  [A loss: 0.900294, acc: 0.468750]\n",
      "192: [D loss: 1.426005, acc: 0.511719]  [A loss: 9.807762, acc: 0.000000]\n",
      "193: [D loss: 0.502369, acc: 0.968750]  [A loss: 0.052530, acc: 1.000000]\n",
      "194: [D loss: 1.533349, acc: 0.501953]  [A loss: 4.034663, acc: 0.003906]\n",
      "195: [D loss: 0.464290, acc: 0.738281]  [A loss: 5.368413, acc: 0.000000]\n",
      "196: [D loss: 0.346926, acc: 0.810547]  [A loss: 6.067809, acc: 0.000000]\n",
      "197: [D loss: 0.230927, acc: 0.882812]  [A loss: 5.415699, acc: 0.000000]\n",
      "198: [D loss: 0.442742, acc: 0.761719]  [A loss: 8.208683, acc: 0.000000]\n",
      "199: [D loss: 0.063780, acc: 0.986328]  [A loss: 4.168402, acc: 0.000000]\n",
      "200: [D loss: 0.719276, acc: 0.634766]  [A loss: 10.511268, acc: 0.000000]\n",
      "201: [D loss: 0.168357, acc: 0.990234]  [A loss: 5.066628, acc: 0.000000]\n",
      "202: [D loss: 0.271371, acc: 0.873047]  [A loss: 5.746845, acc: 0.000000]\n",
      "203: [D loss: 0.193215, acc: 0.925781]  [A loss: 5.583498, acc: 0.000000]\n",
      "204: [D loss: 0.271735, acc: 0.859375]  [A loss: 7.241431, acc: 0.000000]\n",
      "205: [D loss: 0.079787, acc: 0.986328]  [A loss: 4.506056, acc: 0.000000]\n",
      "206: [D loss: 0.517675, acc: 0.724609]  [A loss: 11.077999, acc: 0.000000]\n",
      "207: [D loss: 0.491444, acc: 0.968750]  [A loss: 0.078842, acc: 1.000000]\n",
      "208: [D loss: 1.563259, acc: 0.507812]  [A loss: 6.285480, acc: 0.000000]\n",
      "209: [D loss: 0.112371, acc: 0.968750]  [A loss: 4.102971, acc: 0.003906]\n",
      "210: [D loss: 0.595535, acc: 0.683594]  [A loss: 9.144390, acc: 0.000000]\n",
      "211: [D loss: 0.066385, acc: 0.994141]  [A loss: 0.075714, acc: 0.992188]\n",
      "212: [D loss: 1.967362, acc: 0.505859]  [A loss: 8.215202, acc: 0.000000]\n",
      "213: [D loss: 0.073948, acc: 0.986328]  [A loss: 4.504304, acc: 0.000000]\n",
      "214: [D loss: 0.957501, acc: 0.597656]  [A loss: 12.113916, acc: 0.000000]\n",
      "215: [D loss: 0.217008, acc: 0.986328]  [A loss: 0.004055, acc: 1.000000]\n",
      "216: [D loss: 3.131090, acc: 0.500000]  [A loss: 3.132684, acc: 0.031250]\n",
      "217: [D loss: 1.563264, acc: 0.529297]  [A loss: 10.046076, acc: 0.000000]\n",
      "218: [D loss: 0.091236, acc: 0.966797]  [A loss: 5.181667, acc: 0.000000]\n",
      "219: [D loss: 1.022758, acc: 0.609375]  [A loss: 10.514029, acc: 0.000000]\n",
      "220: [D loss: 0.071246, acc: 0.992188]  [A loss: 4.912143, acc: 0.000000]\n",
      "221: [D loss: 1.230055, acc: 0.603516]  [A loss: 12.146470, acc: 0.000000]\n",
      "222: [D loss: 0.296421, acc: 0.982422]  [A loss: 5.555201, acc: 0.000000]\n",
      "223: [D loss: 0.601318, acc: 0.728516]  [A loss: 9.607113, acc: 0.000000]\n",
      "224: [D loss: 0.124037, acc: 0.964844]  [A loss: 5.108243, acc: 0.007812]\n",
      "225: [D loss: 0.857514, acc: 0.658203]  [A loss: 11.694875, acc: 0.000000]\n",
      "226: [D loss: 0.309576, acc: 0.976562]  [A loss: 0.084909, acc: 0.972656]\n",
      "227: [D loss: 2.404379, acc: 0.500000]  [A loss: 8.432582, acc: 0.000000]\n",
      "228: [D loss: 0.129298, acc: 0.953125]  [A loss: 4.712510, acc: 0.015625]\n",
      "229: [D loss: 0.789212, acc: 0.656250]  [A loss: 9.396205, acc: 0.000000]\n",
      "230: [D loss: 0.083863, acc: 0.966797]  [A loss: 5.140814, acc: 0.003906]\n",
      "231: [D loss: 0.889468, acc: 0.646484]  [A loss: 11.419292, acc: 0.000000]\n",
      "232: [D loss: 0.050954, acc: 0.998047]  [A loss: 5.272483, acc: 0.000000]\n",
      "233: [D loss: 0.653957, acc: 0.703125]  [A loss: 10.506938, acc: 0.000000]\n",
      "234: [D loss: 0.069006, acc: 0.992188]  [A loss: 5.037150, acc: 0.007812]\n",
      "235: [D loss: 0.721171, acc: 0.687500]  [A loss: 11.331534, acc: 0.000000]\n",
      "236: [D loss: 0.082623, acc: 0.992188]  [A loss: 5.443607, acc: 0.000000]\n",
      "237: [D loss: 0.606540, acc: 0.730469]  [A loss: 10.767044, acc: 0.000000]\n",
      "238: [D loss: 0.018632, acc: 0.998047]  [A loss: 5.067042, acc: 0.007812]\n",
      "239: [D loss: 0.556055, acc: 0.751953]  [A loss: 10.557905, acc: 0.000000]\n",
      "240: [D loss: 0.032371, acc: 0.992188]  [A loss: 5.185195, acc: 0.007812]\n",
      "241: [D loss: 0.607812, acc: 0.720703]  [A loss: 11.805377, acc: 0.000000]\n",
      "242: [D loss: 0.356091, acc: 0.978516]  [A loss: 6.203670, acc: 0.000000]\n",
      "243: [D loss: 0.335190, acc: 0.845703]  [A loss: 9.093717, acc: 0.000000]\n",
      "244: [D loss: 0.071302, acc: 0.978516]  [A loss: 5.633009, acc: 0.000000]\n",
      "245: [D loss: 0.539771, acc: 0.746094]  [A loss: 12.942972, acc: 0.000000]\n",
      "246: [D loss: 0.417524, acc: 0.972656]  [A loss: 0.000084, acc: 1.000000]\n",
      "247: [D loss: 5.123766, acc: 0.500000]  [A loss: 1.546383, acc: 0.355469]\n",
      "248: [D loss: 1.934974, acc: 0.513672]  [A loss: 11.119253, acc: 0.000000]\n",
      "249: [D loss: 0.037983, acc: 0.994141]  [A loss: 5.183424, acc: 0.003906]\n",
      "250: [D loss: 0.903829, acc: 0.646484]  [A loss: 11.281849, acc: 0.000000]\n",
      "251: [D loss: 0.053104, acc: 0.986328]  [A loss: 5.502848, acc: 0.000000]\n",
      "252: [D loss: 0.995987, acc: 0.634766]  [A loss: 12.312784, acc: 0.000000]\n",
      "253: [D loss: 0.045038, acc: 0.994141]  [A loss: 5.934118, acc: 0.000000]\n",
      "254: [D loss: 0.833378, acc: 0.671875]  [A loss: 12.208691, acc: 0.000000]\n",
      "255: [D loss: 0.074900, acc: 0.994141]  [A loss: 6.280897, acc: 0.003906]\n",
      "256: [D loss: 0.789281, acc: 0.693359]  [A loss: 12.961872, acc: 0.000000]\n",
      "257: [D loss: 0.150501, acc: 0.988281]  [A loss: 0.004937, acc: 1.000000]\n",
      "258: [D loss: 3.563216, acc: 0.500000]  [A loss: 9.012903, acc: 0.000000]\n",
      "259: [D loss: 0.279677, acc: 0.875000]  [A loss: 7.423637, acc: 0.000000]\n",
      "260: [D loss: 0.856819, acc: 0.660156]  [A loss: 12.772179, acc: 0.000000]\n",
      "261: [D loss: 0.012045, acc: 1.000000]  [A loss: 6.504953, acc: 0.000000]\n",
      "262: [D loss: 1.183527, acc: 0.603516]  [A loss: 15.075715, acc: 0.000000]\n",
      "263: [D loss: 0.838963, acc: 0.945312]  [A loss: 0.000020, acc: 1.000000]\n",
      "264: [D loss: 7.301342, acc: 0.500000]  [A loss: 0.016047, acc: 0.996094]\n",
      "265: [D loss: 3.661710, acc: 0.501953]  [A loss: 4.452358, acc: 0.039062]\n",
      "266: [D loss: 2.706687, acc: 0.511719]  [A loss: 10.750679, acc: 0.000000]\n",
      "267: [D loss: 0.545584, acc: 0.771484]  [A loss: 8.417826, acc: 0.003906]\n",
      "268: [D loss: 1.221251, acc: 0.615234]  [A loss: 11.635294, acc: 0.000000]\n",
      "269: [D loss: 0.265086, acc: 0.873047]  [A loss: 7.366613, acc: 0.000000]\n",
      "270: [D loss: 2.004634, acc: 0.544922]  [A loss: 14.820684, acc: 0.000000]\n",
      "271: [D loss: 0.158116, acc: 0.990234]  [A loss: 10.430676, acc: 0.000000]\n",
      "272: [D loss: 0.713235, acc: 0.753906]  [A loss: 11.556215, acc: 0.000000]\n",
      "273: [D loss: 0.339387, acc: 0.875000]  [A loss: 9.255341, acc: 0.000000]\n",
      "274: [D loss: 1.087733, acc: 0.646484]  [A loss: 14.085711, acc: 0.000000]\n",
      "275: [D loss: 0.006880, acc: 0.998047]  [A loss: 8.239606, acc: 0.003906]\n",
      "276: [D loss: 1.705778, acc: 0.576172]  [A loss: 15.783936, acc: 0.000000]\n",
      "277: [D loss: 1.920731, acc: 0.880859]  [A loss: 13.469625, acc: 0.000000]\n",
      "278: [D loss: 0.040940, acc: 0.982422]  [A loss: 7.099161, acc: 0.011719]\n",
      "279: [D loss: 2.216511, acc: 0.550781]  [A loss: 16.019897, acc: 0.000000]\n",
      "280: [D loss: 4.055077, acc: 0.744141]  [A loss: 0.000005, acc: 1.000000]\n",
      "281: [D loss: 7.603666, acc: 0.500000]  [A loss: 0.008986, acc: 1.000000]\n",
      "282: [D loss: 4.197426, acc: 0.503906]  [A loss: 10.577907, acc: 0.000000]\n",
      "283: [D loss: 2.014474, acc: 0.580078]  [A loss: 13.841925, acc: 0.000000]\n",
      "284: [D loss: 0.052816, acc: 0.978516]  [A loss: 7.003630, acc: 0.015625]\n",
      "285: [D loss: 3.446127, acc: 0.519531]  [A loss: 14.537066, acc: 0.000000]\n",
      "286: [D loss: 0.033202, acc: 0.982422]  [A loss: 9.060750, acc: 0.003906]\n",
      "287: [D loss: 2.500837, acc: 0.560547]  [A loss: 14.519310, acc: 0.000000]\n",
      "288: [D loss: 0.031022, acc: 0.984375]  [A loss: 9.460686, acc: 0.000000]\n",
      "289: [D loss: 2.106446, acc: 0.576172]  [A loss: 15.288792, acc: 0.000000]\n",
      "290: [D loss: 0.064155, acc: 0.996094]  [A loss: 11.740084, acc: 0.000000]\n",
      "291: [D loss: 0.601619, acc: 0.808594]  [A loss: 10.336384, acc: 0.007812]\n",
      "292: [D loss: 1.148007, acc: 0.667969]  [A loss: 14.278873, acc: 0.000000]\n",
      "293: [D loss: 0.003977, acc: 1.000000]  [A loss: 8.782299, acc: 0.003906]\n",
      "294: [D loss: 2.216913, acc: 0.578125]  [A loss: 15.825628, acc: 0.000000]\n",
      "295: [D loss: 0.861121, acc: 0.945312]  [A loss: 0.003069, acc: 1.000000]\n",
      "296: [D loss: 5.606089, acc: 0.500000]  [A loss: 10.240329, acc: 0.000000]\n",
      "297: [D loss: 2.217521, acc: 0.560547]  [A loss: 15.341687, acc: 0.000000]\n",
      "298: [D loss: 0.095633, acc: 0.994141]  [A loss: 12.982965, acc: 0.000000]\n",
      "299: [D loss: 0.560704, acc: 0.818359]  [A loss: 12.162870, acc: 0.000000]\n",
      "300: [D loss: 0.830617, acc: 0.740234]  [A loss: 13.320650, acc: 0.003906]\n",
      "301: [D loss: 0.280985, acc: 0.906250]  [A loss: 9.501230, acc: 0.003906]\n",
      "302: [D loss: 2.765651, acc: 0.548828]  [A loss: 15.944313, acc: 0.000000]\n",
      "303: [D loss: 0.808540, acc: 0.949219]  [A loss: 2.162961, acc: 0.421875]\n",
      "304: [D loss: 5.675405, acc: 0.500000]  [A loss: 15.050487, acc: 0.000000]\n",
      "305: [D loss: 0.040629, acc: 0.994141]  [A loss: 10.554758, acc: 0.000000]\n",
      "306: [D loss: 3.223722, acc: 0.535156]  [A loss: 16.064442, acc: 0.000000]\n",
      "307: [D loss: 1.920691, acc: 0.880859]  [A loss: 15.886848, acc: 0.000000]\n",
      "308: [D loss: 0.222895, acc: 0.984375]  [A loss: 6.806100, acc: 0.062500]\n",
      "309: [D loss: 6.646503, acc: 0.501953]  [A loss: 11.929903, acc: 0.003906]\n",
      "310: [D loss: 2.337843, acc: 0.609375]  [A loss: 16.104506, acc: 0.000000]\n",
      "311: [D loss: 2.298490, acc: 0.857422]  [A loss: 16.080738, acc: 0.000000]\n",
      "312: [D loss: 1.594279, acc: 0.900391]  [A loss: 10.205018, acc: 0.011719]\n",
      "313: [D loss: 5.742518, acc: 0.501953]  [A loss: 15.879985, acc: 0.000000]\n",
      "314: [D loss: 0.125948, acc: 0.992188]  [A loss: 15.607514, acc: 0.000000]\n",
      "315: [D loss: 0.000143, acc: 1.000000]  [A loss: 13.716199, acc: 0.000000]\n",
      "316: [D loss: 0.863988, acc: 0.777344]  [A loss: 15.924124, acc: 0.000000]\n",
      "317: [D loss: 0.257490, acc: 0.982422]  [A loss: 3.029369, acc: 0.320312]\n",
      "318: [D loss: 7.438077, acc: 0.500000]  [A loss: 2.146861, acc: 0.507812]\n",
      "319: [D loss: 7.076208, acc: 0.501953]  [A loss: 10.288403, acc: 0.027344]\n",
      "320: [D loss: 5.847007, acc: 0.505859]  [A loss: 16.118101, acc: 0.000000]\n",
      "321: [D loss: 1.151015, acc: 0.927734]  [A loss: 11.486702, acc: 0.011719]\n",
      "322: [D loss: 5.582262, acc: 0.513672]  [A loss: 16.099987, acc: 0.000000]\n",
      "323: [D loss: 0.913532, acc: 0.943359]  [A loss: 15.765097, acc: 0.000000]\n",
      "324: [D loss: 0.032104, acc: 0.998047]  [A loss: 14.617722, acc: 0.000000]\n",
      "325: [D loss: 0.416825, acc: 0.890625]  [A loss: 14.788216, acc: 0.000000]\n",
      "326: [D loss: 0.172695, acc: 0.949219]  [A loss: 12.980310, acc: 0.003906]\n",
      "327: [D loss: 2.845044, acc: 0.589844]  [A loss: 16.118101, acc: 0.000000]\n",
      "328: [D loss: 4.438777, acc: 0.724609]  [A loss: 16.118101, acc: 0.000000]\n",
      "329: [D loss: 4.691395, acc: 0.707031]  [A loss: 8.060952, acc: 0.054688]\n",
      "330: [D loss: 7.506219, acc: 0.500000]  [A loss: 4.467597, acc: 0.308594]\n",
      "331: [D loss: 7.085931, acc: 0.501953]  [A loss: 11.056067, acc: 0.015625]\n",
      "332: [D loss: 4.980744, acc: 0.515625]  [A loss: 16.118101, acc: 0.000000]\n",
      "333: [D loss: 1.699440, acc: 0.894531]  [A loss: 12.205764, acc: 0.011719]\n",
      "334: [D loss: 3.502437, acc: 0.582031]  [A loss: 16.099813, acc: 0.000000]\n",
      "335: [D loss: 1.127952, acc: 0.929688]  [A loss: 13.480428, acc: 0.003906]\n",
      "336: [D loss: 1.236686, acc: 0.753906]  [A loss: 15.805891, acc: 0.000000]\n",
      "337: [D loss: 0.000008, acc: 1.000000]  [A loss: 15.571470, acc: 0.000000]\n",
      "338: [D loss: 0.000245, acc: 1.000000]  [A loss: 13.653395, acc: 0.007812]\n",
      "339: [D loss: 0.968601, acc: 0.808594]  [A loss: 15.668289, acc: 0.000000]\n",
      "340: [D loss: 0.000724, acc: 1.000000]  [A loss: 14.649776, acc: 0.000000]\n",
      "341: [D loss: 0.155871, acc: 0.939453]  [A loss: 12.396288, acc: 0.003906]\n",
      "342: [D loss: 3.085941, acc: 0.580078]  [A loss: 16.118101, acc: 0.000000]\n",
      "343: [D loss: 1.826079, acc: 0.886719]  [A loss: 16.118101, acc: 0.000000]\n",
      "344: [D loss: 1.511072, acc: 0.906250]  [A loss: 16.118101, acc: 0.000000]\n",
      "345: [D loss: 1.354169, acc: 0.916016]  [A loss: 16.102457, acc: 0.000000]\n",
      "346: [D loss: 0.571393, acc: 0.962891]  [A loss: 0.000418, acc: 1.000000]\n",
      "347: [D loss: 7.452816, acc: 0.500000]  [A loss: 1.747936, acc: 0.617188]\n",
      "348: [D loss: 7.462089, acc: 0.500000]  [A loss: 1.011572, acc: 0.730469]\n",
      "349: [D loss: 7.059330, acc: 0.501953]  [A loss: 11.411510, acc: 0.023438]\n",
      "350: [D loss: 6.710340, acc: 0.503906]  [A loss: 14.970329, acc: 0.000000]\n",
      "351: [D loss: 0.581861, acc: 0.871094]  [A loss: 15.118952, acc: 0.000000]\n",
      "352: [D loss: 0.145490, acc: 0.960938]  [A loss: 11.933583, acc: 0.007812]\n",
      "353: [D loss: 4.902617, acc: 0.513672]  [A loss: 16.103043, acc: 0.000000]\n",
      "354: [D loss: 0.234909, acc: 0.984375]  [A loss: 10.839598, acc: 0.035156]\n",
      "355: [D loss: 6.695969, acc: 0.501953]  [A loss: 13.792107, acc: 0.007812]\n",
      "356: [D loss: 0.988761, acc: 0.787109]  [A loss: 15.720377, acc: 0.000000]\n",
      "357: [D loss: 0.000771, acc: 1.000000]  [A loss: 15.157618, acc: 0.000000]\n",
      "358: [D loss: 0.127226, acc: 0.966797]  [A loss: 12.173936, acc: 0.019531]\n",
      "359: [D loss: 4.509903, acc: 0.521484]  [A loss: 16.118101, acc: 0.000000]\n",
      "360: [D loss: 0.171553, acc: 0.988281]  [A loss: 9.348176, acc: 0.058594]\n",
      "361: [D loss: 7.380593, acc: 0.500000]  [A loss: 5.997303, acc: 0.183594]\n",
      "362: [D loss: 7.779468, acc: 0.500000]  [A loss: 0.239396, acc: 0.917969]\n",
      "363: [D loss: 5.847168, acc: 0.505859]  [A loss: 16.117287, acc: 0.000000]\n",
      "364: [D loss: 0.094442, acc: 0.994141]  [A loss: 16.114761, acc: 0.000000]\n",
      "365: [D loss: 0.094442, acc: 0.994141]  [A loss: 16.118101, acc: 0.000000]\n",
      "366: [D loss: 0.000000, acc: 1.000000]  [A loss: 16.118101, acc: 0.000000]\n",
      "367: [D loss: 0.000000, acc: 1.000000]  [A loss: 16.118101, acc: 0.000000]\n",
      "368: [D loss: 0.062961, acc: 0.996094]  [A loss: 16.107664, acc: 0.000000]\n",
      "369: [D loss: 0.031481, acc: 0.998047]  [A loss: 16.111176, acc: 0.000000]\n",
      "370: [D loss: 0.000000, acc: 1.000000]  [A loss: 16.109634, acc: 0.000000]\n",
      "371: [D loss: 0.031481, acc: 0.998047]  [A loss: 16.105675, acc: 0.000000]\n",
      "372: [D loss: 0.000000, acc: 1.000000]  [A loss: 16.117664, acc: 0.000000]\n",
      "373: [D loss: 0.031481, acc: 0.998047]  [A loss: 16.118101, acc: 0.000000]\n",
      "374: [D loss: 0.031481, acc: 0.998047]  [A loss: 16.079823, acc: 0.000000]\n",
      "375: [D loss: 0.031481, acc: 0.998047]  [A loss: 16.080700, acc: 0.000000]\n",
      "376: [D loss: 0.000000, acc: 1.000000]  [A loss: 16.085075, acc: 0.000000]\n",
      "377: [D loss: 0.000000, acc: 1.000000]  [A loss: 16.029163, acc: 0.000000]\n",
      "378: [D loss: 0.000000, acc: 1.000000]  [A loss: 15.992792, acc: 0.000000]\n",
      "379: [D loss: 0.000022, acc: 1.000000]  [A loss: 15.931118, acc: 0.000000]\n",
      "380: [D loss: 0.000487, acc: 1.000000]  [A loss: 14.649719, acc: 0.000000]\n",
      "381: [D loss: 3.676606, acc: 0.552734]  [A loss: 16.118101, acc: 0.000000]\n",
      "382: [D loss: 7.331298, acc: 0.544922]  [A loss: 16.118101, acc: 0.000000]\n",
      "383: [D loss: 0.094442, acc: 0.994141]  [A loss: 16.118101, acc: 0.000000]\n",
      "384: [D loss: 0.062961, acc: 0.996094]  [A loss: 16.118101, acc: 0.000000]\n",
      "385: [D loss: 0.094442, acc: 0.994141]  [A loss: 16.118101, acc: 0.000000]\n",
      "386: [D loss: 0.168405, acc: 0.988281]  [A loss: 3.607609, acc: 0.402344]\n",
      "387: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "388: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "389: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "390: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "391: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "392: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "393: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "394: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "395: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "396: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "397: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "398: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "399: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "400: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "401: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "402: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "403: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "404: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "405: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "406: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "407: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "408: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "409: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "410: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "411: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "412: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "413: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "414: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "415: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "416: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "417: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "418: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "419: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "420: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "421: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "422: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "423: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "424: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "425: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "426: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "427: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "428: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "429: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "430: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "431: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "432: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "433: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "434: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "435: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "436: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "437: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "438: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "439: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "440: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "441: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "442: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "443: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "444: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "445: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "446: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "447: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "448: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "449: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "450: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "451: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "452: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "453: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "454: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "455: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "456: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "457: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "458: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "459: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "460: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "461: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "462: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "463: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "464: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "465: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "466: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "467: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "468: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "469: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "470: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "471: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "472: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "473: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "474: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "475: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "476: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "477: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "478: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "479: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "480: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "481: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "482: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "483: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "484: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "485: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "486: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "487: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "488: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "489: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "490: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "491: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "492: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "493: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "494: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "495: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "496: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "497: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "498: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "499: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "500: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "Elapsed: 2.4553079139524034 hr \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = mnist.load_data()[0][0]\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows,self.img_cols, 1).astype(np.float32)\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mnist_dcgan = MNIST_DCGAN()\n",
    "    timer = ElapsedTimer()\n",
    "    mnist_dcgan.train(train_steps=501, batch_size=256, save_interval=10)\n",
    "    timer.elapsed_time()\n",
    "    mnist_dcgan.plot_images(fake=True)\n",
    "    mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
